{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a8301bf64d4"
      },
      "source": [
        "# Using the Vertex AI SDK with Large Language Models\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/vertex_sdk_llm_snippets.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "962e636b5cee"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5d108e0f262"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use the Vertex AI SDK to run Large Language Models on Vertex AI via the PaLM API. You will find sample code to test, tune, and deploy generative AI language models. Get started by exploring examples of content summarization, sentiment snalysis, and chat, as well as text embedding and prompt tuning. \n",
        "\n",
        "Learn more about [PaLM API](https://ai.google/discover/palm2/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "825a6b6ffb84"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to provide text input to Large Language Models (LLMs) available on Vertex AI to test, tune, and deploy generative AI language models.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI PaLM API\n",
        "- Vertex AI SDK\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Use the predict endpoints of Vertex AI PaLM API to receive generative AI responses to a message.\n",
        "- Use the text embedding endpoint to receive a vector representation of a message.\n",
        "- Perform prompt tuning of an LLM, based on input/output training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946810630936"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e150bf471f1b"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a85627de3636"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d98bc9fdd80d"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dbf29389c65"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8547fbbb241c"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bc8a29f9001"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e61aaa036444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"northam-ce-mlai-tpu\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4a624c8099d"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f83bd6013894"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08bfd1eb44ef"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af349043f23b"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad1138a125ea"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ce6043da7b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E0123 14:20:10.927488017  341299 backup_poller.cc:127]                 Run client channel backup poller: UNKNOWN:pollset_work {created_time:\"2024-01-23T14:20:10.925731108+00:00\", children:[UNKNOWN:Bad file descriptor {created_time:\"2024-01-23T14:20:10.925675995+00:00\", errno:9, os_error:\"Bad file descriptor\", syscall:\"epoll_wait\"}]}\n",
            "\n",
            "You are running on a Google Compute Engine virtual machine.\n",
            "It is recommended that you use service accounts for authentication.\n",
            "\n",
            "You can run:\n",
            "\n",
            "  $ gcloud config set account `ACCOUNT`\n",
            "\n",
            "to switch accounts if necessary.\n",
            "\n",
            "Your credentials may be visible to others with access to this\n",
            "virtual machine. Are you sure you want to authenticate with\n",
            "your personal account?\n",
            "\n",
            "Do you want to continue (Y/n)?  ^C\n",
            "\n",
            "\n",
            "Command killed by keyboard interrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0367eac06a10"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "21ad4dbb4a61"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13224697bfb"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2fd1f2fd670"
      },
      "source": [
        "## Installation\n",
        "Install the following packages required to execute this notebook.\n",
        "\n",
        "Remember to restart the runtime after installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41df9761a7ea"
      },
      "outputs": [],
      "source": [
        "!pip3 install google-cloud-aiplatform>=1.25 \"shapely<2.0.0\" --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acedaa0065d1"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4b9ee6efcf5c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c9865d6d81fa"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.preview.language_models import (ChatModel, InputOutputTextPair,\n",
        "                                              TextEmbeddingModel,\n",
        "                                              TextGenerationModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ae1b641d63f"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4efe77faaf16"
      },
      "outputs": [],
      "source": [
        "vertexai.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOabLACbseoE"
      },
      "source": [
        "# Summarization examples: transcript summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XmnFXfnhJ9K-"
      },
      "outputs": [
        {
          "ename": "PermissionDenied",
          "evalue": "403 Request had insufficient authentication scopes. [reason: \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"method\"\n  value: \"google.cloud.aiplatform.v1.ModelGardenService.GetPublisherModel\"\n}\nmetadata {\n  key: \"service\"\n  value: \"aiplatform.googleapis.com\"\n}\n]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:65\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1161\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1155\u001b[0m (\n\u001b[1;32m   1156\u001b[0m     state,\n\u001b[1;32m   1157\u001b[0m     call,\n\u001b[1;32m   1158\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1159\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1160\u001b[0m )\n\u001b[0;32m-> 1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/grpc/_channel.py:1004\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
            "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.PERMISSION_DENIED\n\tdetails = \"Request had insufficient authentication scopes.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:209.85.145.95:443 {created_time:\"2024-01-23T14:19:10.992386995+00:00\", grpc_status:7, grpc_message:\"Request had insufficient authentication scopes.\"}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPermissionDenied\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTextGenerationModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-bison@001\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provide a summary with about two sentences for the following article:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mThe efficient-market hypothesis (EMH) is a hypothesis in financial \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse from Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/_model_garden/_model_garden_models.py:214\u001b[0m, in \u001b[0;36m_ModelGardenModel.from_pretrained\u001b[0;34m(cls, model_name)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_INSTANCE_SCHEMA_URI:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a correct model interface class since it does not have an instance schema URI.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m     )\n\u001b[0;32m--> 214\u001b[0m model_info \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_to_class_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_INSTANCE_SCHEMA_URI\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(model_info\u001b[38;5;241m.\u001b[39minterface_class, \u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_info\u001b[38;5;241m.\u001b[39minterface_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m     )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/vertexai/_model_garden/_model_garden_models.py:96\u001b[0m, in \u001b[0;36m_get_model_info\u001b[0;34m(model_id, schema_to_class_map)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_id:\n\u001b[1;32m     93\u001b[0m     model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublishers/google/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_id\n\u001b[1;32m     95\u001b[0m publisher_model_res \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 96\u001b[0m     \u001b[43m_publisher_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_PublisherModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_gca_resource\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m publisher_model_res\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublishers/google/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly Google models are currently supported. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublisher_model_res\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/_publisher_models.py:77\u001b[0m, in \u001b[0;36m_PublisherModel.__init__\u001b[0;34m(self, resource_name, project, location, credentials)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is not a valid PublisherModel resource \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname or model garden id.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getter_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_resource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_DEFAULT_RETRY\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/model_garden_service/client.py:536\u001b[0m, in \u001b[0;36mModelGardenServiceClient.get_publisher_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    531\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    532\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mname),)),\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:67\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[0;31mPermissionDenied\u001b[0m: 403 Request had insufficient authentication scopes. [reason: \"ACCESS_TOKEN_SCOPE_INSUFFICIENT\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"method\"\n  value: \"google.cloud.aiplatform.v1.ModelGardenService.GetPublisherModel\"\n}\nmetadata {\n  key: \"service\"\n  value: \"aiplatform.googleapis.com\"\n}\n]"
          ]
        }
      ],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Provide a summary with about two sentences for the following article:\n",
        "The efficient-market hypothesis (EMH) is a hypothesis in financial \\\n",
        "economics that states that asset prices reflect all available \\\n",
        "information. A direct implication is that it is impossible to \\\n",
        "\"beat the market\" consistently on a risk-adjusted basis since market \\\n",
        "prices should only react to new information. Because the EMH is \\\n",
        "formulated in terms of risk adjustment, it only makes testable \\\n",
        "predictions when coupled with a particular model of risk. As a \\\n",
        "result, research in financial economics since at least the 1990s has \\\n",
        "focused on market anomalies, that is, deviations from specific \\\n",
        "models of risk. The idea that financial market returns are difficult \\\n",
        "to predict goes back to Bachelier, Mandelbrot, and Samuelson, but \\\n",
        "is closely associated with Eugene Fama, in part due to his \\\n",
        "influential 1970 review of the theoretical and empirical research. \\\n",
        "The EMH provides the basic logic for modern risk-based theories of \\\n",
        "asset prices, and frameworks such as consumption-based asset pricing \\\n",
        "and intermediary asset pricing can be thought of as the combination \\\n",
        "of a model of risk with the EMH. Many decades of empirical research \\\n",
        "on return predictability has found mixed evidence. Research in the \\\n",
        "1950s and 1960s often found a lack of predictability (e.g. Ball and \\\n",
        "Brown 1968; Fama, Fisher, Jensen, and Roll 1969), yet the \\\n",
        "1980s-2000s saw an explosion of discovered return predictors (e.g. \\\n",
        "Rosenberg, Reid, and Lanstein 1985; Campbell and Shiller 1988; \\\n",
        "Jegadeesh and Titman 1993). Since the 2010s, studies have often \\\n",
        "found that return predictability has become more elusive, as \\\n",
        "predictability fails to work out-of-sample (Goyal and Welch 2008), \\\n",
        "or has been weakened by advances in trading technology and investor \\\n",
        "learning (Chordia, Subrahmanyam, and Tong 2014; McLean and Pontiff \\\n",
        "2016; Martineau 2021).\n",
        "Summary:\n",
        "\"\"\",\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=256,\n",
        "    top_k=40,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdxJX2dNE7t3"
      },
      "source": [
        "# Classification examples: classification headline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2fw1_3H-tjX"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"What is the topic for a given news headline?\n",
        "- business\n",
        "- entertainment\n",
        "- health\n",
        "- sports\n",
        "- technology\n",
        "\n",
        "Text: Pixel 7 Pro Expert Hands On Review, the Most Helpful Google Phones.\n",
        "The answer is: technology\n",
        "\n",
        "Text: Quit smoking?\n",
        "The answer is: health\n",
        "\n",
        "Text: Roger Federer reveals why he touched Rafael Nadals hand while they were crying\n",
        "The answer is: sports\n",
        "\n",
        "Text: Business relief from Arizona minimum-wage hike looking more remote\n",
        "The answer is: business\n",
        "\n",
        "Text: #TomCruise has arrived in Bari, Italy for #MissionImpossible.\n",
        "The answer is: entertainment\n",
        "\n",
        "Text: CNBC Reports Rising Digital Profit as Print Advertising Falls\n",
        "The answer is:\n",
        "\"\"\",\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=5,\n",
        "    top_k=1,\n",
        "    top_p=0,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKsuhKli70q-"
      },
      "source": [
        "# Classification examples: sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipqROPufuhDr"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"I had to compare two versions of Hamlet for my Shakespeare class and \\\n",
        "unfortunately I picked this version. Everything from the acting (the actors \\\n",
        "deliver most of their lines directly to the camera) to the camera shots (all \\\n",
        "medium or close up shots...no scenery shots and very little back ground in the \\\n",
        "shots) were absolutely terrible. I watched this over my spring break and it is \\\n",
        "very safe to say that I feel that I was gypped out of 114 minutes of my \\\n",
        "vacation. Not recommended by any stretch of the imagination.\n",
        "Classify the sentiment of the message: negative\n",
        "\n",
        "Something surprised me about this movie - it was actually original. It was not \\\n",
        "the same old recycled crap that comes out of Hollywood every month. I saw this \\\n",
        "movie on video because I did not even know about it before I saw it at my \\\n",
        "local video store. If you see this movie available - rent it - you will not \\\n",
        "regret it.\n",
        "Classify the sentiment of the message: positive\n",
        "\n",
        "My family has watched Arthur Bach stumble and stammer since the movie first \\\n",
        "came out. We have most lines memorized. I watched it two weeks ago and still \\\n",
        "get tickled at the simple humor and view-at-life that Dudley Moore portrays. \\\n",
        "Liza Minelli did a wonderful job as the side kick - though I\\'m not her \\\n",
        "biggest fan. This movie makes me just enjoy watching movies. My favorite scene \\\n",
        "is when Arthur is visiting his fiancée\\'s house. His conversation with the \\\n",
        "butler and Susan\\'s father is side-spitting. The line from the butler, \\\n",
        "\"Would you care to wait in the Library\" followed by Arthur\\'s reply, \\\n",
        "\"Yes I would, the bathroom is out of the question\", is my NEWMAIL \\\n",
        "notification on my computer.\n",
        "Classify the sentiment of the message: positive\n",
        "\n",
        "This Charles outing is decent but this is a pretty low-key performance. Marlon \\\n",
        "Brando stands out. There\\'s a subplot with Mira Sorvino and Donald Sutherland \\\n",
        "that forgets to develop and it hurts the film a little. I\\'m still trying to \\\n",
        "figure out why Charlie want to change his name.\n",
        "Classify the sentiment of the message: negative\n",
        "\n",
        "Tweet: The Pixel 7 Pro, is too big to fit in my jeans pocket, so I bought \\\n",
        "new jeans.\n",
        "Classify the sentiment of the message: \"\"\",\n",
        "    max_output_tokens=5,\n",
        "    temperature=0.2,\n",
        "    top_k=1,\n",
        "    top_p=0,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh0tLRggE5H1"
      },
      "source": [
        "# Extraction examples: extractive question answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPqZ38QDoBeh"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"\"\"Background: There is evidence that there have been significant changes \\\n",
        "in Amazon rainforest vegetation over the last 21,000 years through the Last \\\n",
        "Glacial Maximum (LGM) and subsequent deglaciation. Analyses of sediment \\\n",
        "deposits from Amazon basin paleo lakes and from the Amazon Fan indicate that \\\n",
        "rainfall in the basin during the LGM was lower than for the present, and this \\\n",
        "was almost certainly associated with reduced moist tropical vegetation cover \\\n",
        "in the basin. There is debate, however, over how extensive this reduction \\\n",
        "was. Some scientists argue that the rainforest was reduced to small, isolated \\\n",
        "refugia separated by open forest and grassland; other scientists argue that \\\n",
        "the rainforest remained largely intact but extended less far to the north, \\\n",
        "south, and east than is seen today. This debate has proved difficult to \\\n",
        "resolve because the practical limitations of working in the rainforest mean \\\n",
        "that data sampling is biased away from the center of the Amazon basin, and \\\n",
        "both explanations are reasonably well supported by the available data.\n",
        "\n",
        "Q: What does LGM stands for?\n",
        "A: Last Glacial Maximum.\n",
        "\n",
        "Q: What did the analysis from the sediment deposits indicate?\n",
        "A: Rainfall in the basin during the LGM was lower than for the present.\n",
        "\n",
        "Q: What are some of scientists arguments?\n",
        "A: The rainforest was reduced to small, isolated refugia separated by open forest and grassland.\n",
        "\n",
        "Q: There have been major changes in Amazon rainforest vegetation over the last how many years?\n",
        "A: 21,000.\n",
        "\n",
        "Q: What caused changes in the Amazon rainforest vegetation?\n",
        "A: The Last Glacial Maximum (LGM) and subsequent deglaciation\n",
        "\n",
        "Q: What has been analyzed to compare Amazon rainfall in the past and present?\n",
        "A: Sediment deposits.\n",
        "\n",
        "Q: What has the lower rainfall in the Amazon during the LGM been attributed to?\n",
        "A:\"\"\",\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=256,\n",
        "    top_k=1,\n",
        "    top_p=0,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV08j9H-Rbr7"
      },
      "source": [
        "# Ideation examples: interview questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofBsWYxt_cl3"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "response = model.predict(\n",
        "    \"Give me ten interview questions for the role of program manager.\",\n",
        "    temperature=0.2,\n",
        "    max_output_tokens=256,\n",
        "    top_k=40,\n",
        "    top_p=0.8,\n",
        ")\n",
        "print(f\"Response from Model: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpeXOD_bsCAX"
      },
      "source": [
        "# Chat examples: science tutoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1my0ebNpsKGT"
      },
      "outputs": [],
      "source": [
        "chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 40,\n",
        "}\n",
        "\n",
        "chat = chat_model.start_chat(\n",
        "    context=\"My name is Miles. You are an astronomer, knowledgeable about the solar system.\",\n",
        "    examples=[\n",
        "        InputOutputTextPair(\n",
        "            input_text=\"How many moons does Mars have?\",\n",
        "            output_text=\"The planet Mars has two moons, Phobos and Deimos.\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"How many planets are there in the solar system?\", **parameters\n",
        ")\n",
        "response = chat.send_message(\n",
        "    \"When I learned about the planets in school, there were nine. When did that change?\",\n",
        "    **parameters,\n",
        ")\n",
        "response = chat.send_message(\n",
        "    \"Does Pluto have any moons? What about other dwarf planets?\", **parameters\n",
        ")\n",
        "response = chat.send_message(\"Who chose all of these cool names?!\", **parameters)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag5Og6Y3xfvM"
      },
      "source": [
        "# Text embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNstE8FOxmm2"
      },
      "outputs": [],
      "source": [
        "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
        "embeddings = model.get_embeddings([\"What is life?\"])\n",
        "for embedding in embeddings:\n",
        "    vector = embedding.values\n",
        "    print(f\"Length of Embedding Vector: {len(vector)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI-dpBTr6LGH"
      },
      "source": [
        "# List tuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-MuqTjTA0zk"
      },
      "outputs": [],
      "source": [
        "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "tuned_model_names = model.list_tuned_model_names()\n",
        "print(tuned_model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYvbJ9PrFF2g"
      },
      "source": [
        "# Tune a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoEVAfGCFOnT"
      },
      "outputs": [],
      "source": [
        "def tuning(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    model_display_name: str,\n",
        "    training_data: pd.DataFrame,\n",
        "    train_steps: int = 10,\n",
        ") -> TextGenerationModel:\n",
        "    \"\"\"Tune a new model, based on a prompt-response data.\n",
        "\n",
        "    \"training_data\" can be either the GCS URI of a file formatted in JSONL format\n",
        "    (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\n",
        "    DataFrame. Each training example should be JSONL record with two keys, for\n",
        "    example:\n",
        "      {\n",
        "        \"input_text\": <input prompt>,\n",
        "        \"output_text\": <associated output>\n",
        "      },\n",
        "    or the pandas DataFame should contain two columns:\n",
        "      ['input_text', 'output_text']\n",
        "    with rows for each training example.\n",
        "\n",
        "    Args:\n",
        "      project_id: GCP Project ID, used to initialize vertexai\n",
        "      location: GCP Region, used to initialize vertexai\n",
        "      model_display_name: Customized Tuned LLM model name.\n",
        "      training_data: GCS URI of jsonl file or pandas dataframe of training data\n",
        "      train_steps: Number of training steps to use when tuning the model.\n",
        "    \"\"\"\n",
        "    vertexai.init(project=project_id, location=location)\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "\n",
        "    model.tune_model(\n",
        "        training_data=training_data,\n",
        "        # Optional:\n",
        "        model_display_name=model_display_name,\n",
        "        train_steps=train_steps,\n",
        "        tuning_job_location=\"europe-west4\",\n",
        "        tuned_model_location=location,\n",
        "    )\n",
        "\n",
        "    print(model._job.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acc05a4e4fe5"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "vertex_sdk_llm_snippets.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
